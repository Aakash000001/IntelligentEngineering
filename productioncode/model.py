# -*- coding: utf-8 -*-
"""human-activity-recognition-with-smartphones-99-acc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kTFsKXfRV45lhDfJrwmiVbF5sK7oWjQj

# Import library
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

import os
for dirname, _, filenames in os.walk('/IntelligentEngineering/'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# Load The train dataset"""

df_train=pd.read_csv('/IntelligentEngineering/train.csv')

df_train.head()

df_train.tail()

"""# Now EDA Part Starting"""

df_train.shape

df_train.isnull().sum()

"""# Class Distribution"""

df_train['Activity'].unique()

"""# Now Visualize the class Distribution"""

plt.figure(figsize=(12,6))
axis=sns.countplot(x="Activity",data=df_train)
plt.xticks(x=df_train['Activity'],rotation='vertical')
plt.show()

"""# Now Subject Part"""

df_train['subject'].unique()

X=pd.DataFrame(df_train.drop(['Activity','subject'],axis=1))
y=df_train.Activity.values.astype(object)

X.shape , y.shape

X.head()

y[5]

X.info()

#Total Number of Continous and Categorical features in the training set
num_cols = X._get_numeric_data().columns
print("Number of numeric features:",num_cols.size)

"""# Transforming Non numerical Labels into numerical labels"""

pip install scikit-learn

from sklearn import preprocessing

encoder=preprocessing.LabelEncoder()

encoder.fit(y)
y=encoder.transform(y)
y.shape

y[5]

encoder.classes_

encoder.classes_[5]

"""# Feature Scaling"""

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()

X=scaler.fit_transform(X)

X[5]

"""# Now Split X and y
# Training and Validation sets
"""

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=100)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""# Now Train The Model

1. SVM
2. Random Forest
"""

# import SVC classifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# import metrics to compute accuracy (Evulate)
from sklearn.metrics import accuracy_score, confusion_matrix,classification_report
from sklearn.model_selection import cross_val_score, GridSearchCV

"""# Firstly SVM

# Run SVM with default hyperparameters
"""

svc=SVC()

svc.fit(X_train,y_train)

y_pred=svc.predict(X_test)

"""# Default SVM Score"""

# compute and print accuracy score
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

"""# Now Some Hyperparameter where kernel use rbf and C=100.0"""

svc2=SVC(kernel='rbf',C=100.0)


# fit classifier to training set
svc2.fit(X_train,y_train)

# make predictions on test set
y_pred2 = svc2.predict(X_test)

# compute and print accuracy score
print('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred2)))

"""# Now Random Forest Classifier"""

rand_clf=RandomForestClassifier(random_state=5)

rand_clf.fit(X_train,y_train)

# compute and print accuracy score
rand_clf.score(X_test,y_test)

"""# Hyperparameters of Random Forest Classifier:

# These are the parameters

1. max_depth: The max_depth of a tree in Random Forest is defined as the longest path between the root node and the leaf node.
2. min_sample_split: Parameter that tells the decision tree in a random forest the minimum required number of observations in any given node to split it. Default = 2
3. max_leaf_nodes: This hyperparameter sets a condition on the splitting of the nodes in the tree and hence restricts the growth of the tree.
4. min_samples_leaf: This Random Forest hyperparameter specifies the minimum number of samples that should be present in the leaf node after splitting a node. Default = 1
5. n_estimators: Number of trees in the forest.
6. max_sample: The max_samples hyperparameter determines what fraction of the original dataset is given to any individual tree.
7. max_features: This resembles the number of maximum features provided to each tree in a random forest.
8. bootstrap: Method for sampling data points (with or without replacement). Default = True
9. criterion: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.

# Now, manually setting the hyperparameters, and using GridSearchCV for Hyperparameter Tuning:

"""

grid_param={
    'n_estimators':[90,100,115,130],
    'criterion':['gini','entropy'],
    'max_depth':range(2,20,1),
    'min_samples_leaf':range(1,10,1),
    'min_samples_split':range(2,10,1),
    'max_features':['auto','log2']
}

grid_search=GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs=-1,verbose=3)

#grid_search.fit(X_train,y_train)

#grid_search.best_params_

#rand_clf=RandomForestClassifier()